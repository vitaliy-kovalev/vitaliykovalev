[{"content":" Hello! I am Vitaliy üëã I'm a Business Intelligence Leader working for Yandex Fintech in Belgrade, Serbia.\nAt work my primary focus is to implement advanced BI solutions and processes, driving data-informed strategies that propel organizational growth. In my own time, I'm a happy father and husband and a board games geek.\nI'm always eager to connect and explore opportunities for collaboration! ü§ù\n","date":null,"permalink":"https://vitaliykovalev.com/","section":"","summary":"","title":""},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/automation/","section":"Tags","summary":"","title":"Automation"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/bi/","section":"Tags","summary":"","title":"BI"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/blog/","section":"Tags","summary":"","title":"Blog"},{"content":" Here is my thoughts on various topics from visualisation techniques to management stuff. There are some tags to help you navigate. Don\u0026rsquo;t forget to subscribe on RSS or follow me on LinkedIn to get notifications about my new posts! ","date":null,"permalink":"https://vitaliykovalev.com/posts/","section":"Blog \u0026 Projects","summary":"","title":"Blog \u0026 Projects"},{"content":" Disclaimer: Models and their limits evolve, but the core approach in this article should remain relevant. Always double-check the current model landscape before implementing. üé• Here‚Äôs a video of me trying to explain all this at the last Serbian Tableau User Group.\nWhat are the prerequisites for using this approach? # The company‚Äôs BI tool has hundreds of reports and continues to grow The company is growing fast, with new employees and new dashboards being added There is a lack of business intelligence culture among dashboard creators, as many reports lack descriptions, tags, and keywords The process of dashboard creation is both rapid and decentralized, allowing for faster development but also introducing challenges in maintaining consistency Documentation exists in-place, meaning small texts near charts or elements next to dashboard filters that explain metrics What is the problem? #End users struggle to find useful information quickly. This typically happens because of BI tool limitations:\nPoor search experience, with no ‚Äúsearch by meaning‚Äù functionality Vendors often sell search improvements as add-ons instead of improving them for free, leading to significant additional costs for extended licenses A bias toward English in BI tool search functionality All of this leads to new ad-hoc tasks for analysts and worsens data literacy across the company. Archiving outdated dashboards may help, but it‚Äôs often not enough.\nWhat is RAG? #RAG (Retrieval-Augmented Generation) is an approach that helps an LLM reduce hallucinations by retrieving information from document storage instead of relying solely on its internal knowledge.\nRAG mitigates hallucinations but doesn‚Äôt eliminate them completely: always validate outputs\nBasic RAG There is basically 2 steps to implement RAG:\nRetrival. Question and documents will be converted into a vectors (embeddings) and then similarity search will be excecuted. Basic search algorithm is cosine similarity ‚Äî a metric that measures how closely two vectors ‚Äúpoint‚Äù in the same direction. The closer the directions, the more similar the meanings. Generation. The k most relevant documents and the initial question are passed to the model for answer generation (along with a custom prompt). Why LLM is better than basic BI search? #Most BI tools I‚Äôve seen in production use a simple search that matches only keywords. In some cases, there are no search capabilities at all.\nLLM, on the other hand, understands meaning ‚Äî even with typos, missing tags, or vague questions ‚Äî making it far more reliable for the complexity and unpredictability of BI environments.\nWhat data and prompt do you need? #1. Reports metadata #A simple text or markdown document that contains the report page content and some metadata, which will be useful later when splitting it into pieces.\nDataLens, my main BI system, provides an API that I use to gather information about dashboards, tabs, and charts on those tabs.\nSimple document template Metadata for this document:\n\u0026#34;meta\u0026#34;: { \u0026#34;domain\u0026#34;: \u0026#34;01 Sales\u0026#34;, \u0026#34;tab_name\u0026#34;: \u0026#34;Some Overview / Main\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://datalens.yandex.cloud/\u0026lt;dash_id\u0026gt;?tab=\u0026lt;tab_id\u0026gt;\u0026#34;, \u0026#34;number_of_views\u0026#34;: 100 } 2. Any LLM API #Open-source models can be very effective. In my production I use an internally hosted DeepSeek V3 API, but you might prefer Mistral or the OpenAI API if your dashboard metadata is not protected by an NDA.\n3. Some general business overview #Just a short text containing information about your business to provide additional context to the LLM. Keep it as brief as possible.\n### Who are we? Dunder Mifflin Paper Company, Inc. is a paper and office supplies wholesale company. There is 13 branches in the US (yet). ### Key Products - Paperclips: it\u0026#39;s the best one - Paper: A4, A2 - Office Supplies: furniture of various brands - ... ### Metrics / Abbriviations - GMV: Gross Merchandise Value - MAU: Monthly Active Users - DHJ: Dwight Hates Jim - ... ### Other Useful Information - Scranton branch is the best on sales - ... 4. Vector database #I use local files and FAISS, but for production environments with many users, you could use a variety of compatible vector databases.\n5. Metadata about user / company‚Äôs intranet API (optional) #If you have intranet portal (such as SharePoint) that list users and the department hierarchy, this information can help improve context-based search and support final decision-making.\n6. Any interface to hand over results to the end users (optional) #In my solution I use a Telegram bot (it‚Äôs widely used messenger within my company), but you can plug results into whatever tool your users prefer. In this article and GitHub repository I use examples of CLI-tools, but then you could easily adapt it to any other interface.\nMy bot answer (you can ask question on any language!) Creating a vectorstore #Now let‚Äôs put it all together! To build an MVP for search functionality, the first step is to create a vector store using the Reports Metadata. The goal of this step is to convert our metadata about dashboards into vectors, also known as embeddings.\nThere are many embedding models available, but a good starting point is intfloat/multilingual-e5-base ‚Äî it‚Äôs CPU-optimized, supports multiple languages, and handles up to 512 tokens, making it a solid choice for processing longer text blocks.\nHowever, if you look at the average metadata report document, it‚Äôs usually much larger than 512 tokens.\nTo address this, it is necessary to split documents into smaller parts that fit within the token limit.\nTo avoid losing important information and to improve the quality of vector search, headers (## and ### ) can be used as natural split points.\nAdditionally, for every chunk text that does not start with a ## About this tab: (which marks the beginning of the document and contains general metadata), extra metadata prepared earlier during the first step of data preparation will be attached.\n--- PART --- Domain: \u0026lt;Domain of the original Document\u0026gt; Tab Name: \u0026lt;Tab Name of the original Document\u0026gt; URL: \u0026lt;URL of the original Document\u0026gt; Number of views: \u0026lt;Number of views of the original Document\u0026gt; --- \u0026lt;chunk content\u0026gt; Here is a simple example of the document (you could have thousands of them):\n{ \u0026#34;meta\u0026#34;: { \u0026#34;domain\u0026#34;: \u0026#34;01 Sales\u0026#34;, \u0026#34;tab_name\u0026#34;: \u0026#34;Regional Dynamics\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://dl.ya.cloud/abc?tab=100\u0026#34;, \u0026#34;number_of_views\u0026#34;: 278 }, \u0026#34;md\u0026#34;: \u0026#34;## About this tab:\\nDomain: 01 Sales\\nTab Name: Regional Dynamics...\u0026#34; } Here is what the creation of the local vector storage looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 import json from langchain.schema import Document from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.vectorstores import FAISS from langchain_huggingface import HuggingFaceEmbeddings from transformers import AutoTokenizer import hashlib def convert( base: list[dict] ) -\u0026gt; list[Document]: \u0026#34;\u0026#34;\u0026#34; Basic function that converts text to langchain.Documents \u0026#34;\u0026#34;\u0026#34; docs = [] for row in base: text = row.get(\u0026#39;md\u0026#39;) metadata = row.get(\u0026#39;meta\u0026#39;) docs.append(Document(page_content=text, metadata=metadata)) return docs def enrich_with_metadata(doc: Document) -\u0026gt; Document: \u0026#34;\u0026#34;\u0026#34; Function that add additional metadata to the document chunk \u0026#34;\u0026#34;\u0026#34; meta = doc.metadata if \u0026#34;source\u0026#34; in meta and \u0026#34;start_index\u0026#34; in meta: meta[\u0026#34;uid\u0026#34;] = f\u0026#34;{meta[\u0026#39;source\u0026#39;]}:{meta[\u0026#39;start_index\u0026#39;]}\u0026#34; else: meta[\u0026#34;uid\u0026#34;] = hashlib.md5( doc.page_content.encode(\u0026#34;utf-8\u0026#34;) ).hexdigest() meta_text = \u0026#39;\\n\u0026#39;.join([ \u0026#34;--- PART ---\u0026#34;, f\u0026#34;Domain: {doc.metadata.get(\u0026#39;domain\u0026#39;, \u0026#39;\u0026#39;)}\u0026#34;, f\u0026#34;Tab Name: {doc.metadata.get(\u0026#39;tab_name\u0026#39;, \u0026#39;-\u0026#39;)}\u0026#34;, f\u0026#34;URL: {doc.metadata.get(\u0026#39;url\u0026#39;, \u0026#39;-\u0026#39;)}\u0026#34;, f\u0026#34;Number of views: {doc.metadata.get(\u0026#39;number_of_views\u0026#39;, 0)}\u0026#34;, \u0026#39;---\u0026#39; ]).strip() if \u0026#39;## About this tab:\u0026#39; in doc.page_content: concatenation = doc.page_content else: concatenation = meta_text + \u0026#34;\\n\u0026#34; + doc.page_content return Document( page_content=concatenation, metadata=meta ) # Launch parameters metadata_path=\u0026#39;./docs\u0026#39; # Path to the folder with test metadata hf_embedding_name=\u0026#39;intfloat/multilingual-e5-base\u0026#39; # Model name vectorstore_output=\u0026#34;./rag # Where to save FAISS vector base chunk_size=512 # Embedding model limitation (depends on model) add_meta_size=90 # Additional meta tokens cnt before the next chunk # Step 1: Getting base of documents and metadata base = [] for filename in os.listdir(metadata_path): if filename.endswith(\u0026#34;.json\u0026#34;): file_path = os.path.join(metadata_path, filename) with open(file_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: try: base.append(json.load(f)) except json.JSONDecodeError as e: print(f\u0026#34;Error in file {filename}: {e}\u0026#34;) docs = convert(base) # Step 2: Splitting documents and enriching chunks with additional meta print(f\u0026#34;Got {len(docs)} documents. Splitting...\u0026#34;) tokenizer = AutoTokenizer.from_pretrained(hf_embedding_name) splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer( tokenizer, chunk_size=int(chunk_size - add_meta_size), chunk_overlap=0, separators=[\u0026#34;\\n## \u0026#34;, \u0026#34;\\n### \u0026#34;], strip_whitespace=True, add_start_index=True, keep_separator=\u0026#34;start\u0026#34; ) chunks = splitter.split_documents(docs) chunks = [enrich_with_metadata(chunk) for chunk in chunks] print(f\u0026#34;üî™ Split into {len(chunks)} chunks\u0026#34;) too_big_details = [] for doc in chunks: if len(tokenizer.encode(doc.page_content)) \u0026gt; chunk_size: too_big_details.append( f\u0026#34;{doc.metadata.get(\u0026#39;url\u0026#39;)}: {len(doc.page_content)}\u0026#34; f\u0026#34; / {chunk_size}\u0026#34; ) too_big_cnt = len(too_big_details) if too_big_cnt \u0026gt; 0: print(f\u0026#34;üî™ {round(float(too_big_cnt / len(chunks)) * 100, 2)}% \u0026#34; f\u0026#34;({too_big_cnt} / {len(chunks)}) chunks is too big \u0026#34; f\u0026#34;for context of {chunk_size} tokens!\u0026#34;) print(\u0026#34;üî™ Check those dashboard tabs: \u0026#34;) print(\u0026#39;\\n\u0026#39;.join(list(set(too_big_details)))) # Step 3: Vectorstore creation embedding_model = HuggingFaceEmbeddings( model_name=hf_embedding_name, encode_kwargs={ \u0026#34;normalize_embeddings\u0026#34;: True } ) vectorstore = FAISS.from_documents(chunks, embedding_model) vectorstore.save_local(vectorstore_output) Now take a closer look at step 2 in the code. As you can see, I used chunk_size of 512 - 90 = 422 tokens and a chunk_overlap of 0 tokens. Why?\nSince every extra metadata block adds about 90 tokens (you can check this using transformers.AutoTokenizer), I needed to reduce the size of every chunk starting from second chunk to stay within the limit for the main text content.\nThis setup should fit the embedding model‚Äôs limit of 512 tokens. However, because metadata can sometimes be messy, in step 4, I checked the quality of the split (the percentage of chunks that would be truncated due to the model‚Äôs limits). To reduce this number, you could use other separators and split long texts into parts. In my case, 1% of the chunks were larger than 512 tokens. This means that in those chunks, the context was truncated.\nNow you have vector storage. What‚Äôs next? #Let‚Äôs build an MVP of the reply engine. There are two more things needed: a good prompt for generation part and the vector store retriever.\nNow let‚Äôs take a closer look at the retriever code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 import os from jinja2 import Environment, FileSystemLoader from langchain.chains.combine_documents import \\ create_stuff_documents_chain from langchain_openai import ChatOpenAI from langchain_community.vectorstores import FAISS from langchain_core.prompts import PromptTemplate from langchain_huggingface import HuggingFaceEmbeddings os.environ[\u0026#34;TOKENIZERS_PARALLELISM\u0026#34;] = \u0026#34;false\u0026#34; def render_template( env: Environment, template_name: str, **kwargs ) -\u0026gt; str: template = env.get_template(template_name) return template.render(**kwargs) # Launch parameters question = \u0026#39;Which report contains table with sales?\u0026#39; retriever_k = 40 vectorstore: \u0026#39;./rag\u0026#39; prompt_location = \u0026#39;./\u0026#39; hf_embedding_name = \u0026#39;intfloat/multilingual-e5-base\u0026#39; # Jinja2 Environment to load the prompt text and expand # it with additional context later on jinja_loader = FileSystemLoader(prompt_location) jinja_env = Environment(loader=jinja_loader) # Same model that have been used in vectorstore creation embedding_model = HuggingFaceEmbeddings( model_name=hf_embedding_name, encode_kwargs={\u0026#34;normalize_embeddings\u0026#34;: True} ) # 1. Similarity search stage db = FAISS.load_local( folder_path=vectorstore, embeddings=embedding_model, allow_dangerous_deserialization=True ) retriever = db.as_retriever( search_kwargs={\u0026#34;k\u0026#34;: retriever_k} ) unique_docs = retriever.invoke(question) print(f\u0026#34;Unique docs cnt: {len(unique_docs)}\u0026#34;) # 2. Generation stage # Initializing the llm api (temperature=0 means that # gpt would be less creative in the answers) llm = ChatOpenAI( model=\u0026#34;gpt-4o\u0026#34;, openai_api_key=os.environ.get(\u0026#39;OPENAI_API_KEY\u0026#39;, \u0026#39;\u0026#39;), temperature=0.0 ) # Reading prompt prompt = render_template( env=jinja_env, template_name=\u0026#39;prompt.md\u0026#39;, ) # Passing our k documents to the prompt final_prompt = PromptTemplate( input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;], template=prompt ) # LLM call chain = create_stuff_documents_chain( llm, final_prompt, document_variable_name=\u0026#34;context\u0026#34; ) result = chain.invoke( { \u0026#34;question\u0026#34;: question, \u0026#34;context\u0026#34;: unique_docs } ) # 3. Result print(f\u0026#34;\\n{result}\\n\u0026#34;) Let‚Äôs break this down:\nFirst, I initialized the HuggingFaceEmbeddings class to define which embedding model I will use during the retrieval process.\nOne important detail: encode_kwargs={\u0026quot;normalize_embeddings\u0026quot;: True}. I used this because FAISS performs better on normalized vectors.\nThen, I created the FAISS database with allow_dangerous_deserialization=True, since Pickle files could contain malware if tampered with. But since I created the files myself, I trust them.\nThen I invoke the retriever to use cosine similarity to find the most relevant docs.\nWhat is cosine similarity? #Let‚Äôs say you want to make a Tinder for sentences: with cosine similarity it would be a piece of cake to find a match!\nLet‚Äôs say you‚Äôre dog lover and would kill for coffee. You have those profiles in the database: 1,2,3. If you apply this algorithm it would say that 1 and 3 is the high match, and number 2 is completely off.\nSimple example of cosine similarity If you imagine your user question and your chunks as the coordinates of meaning in multidimensional space then you could just compare the align and direction of those vectors to find the most similar.\nNow let‚Äôs talk about search_kwargs.k parameter.\nThis parameter controls the number of documents the retriever will return after finding results cosine-similar to the user‚Äôs query.\nSince I use in code example gpt-4o the context window is 128k tokens (up to 300 chunks), but in production I use DeepSeek V3, where the context window is only 64 000 tokens, meaning that I could theoretically pass around 150 document chunks.\nHowever, keep in mind you also need room for the system prompt ‚Äî the more system context you add, the fewer document chunks you can pass without hitting limits.\nYou should also remember:\nHigher k ‚Üí more documents ‚Üí better quality, but Higher k ‚Üí longer execution time. So, you‚Äôll need to find a balance. I recommend starting with k = 40 and adjusting based on your needs.\nHow to measure quality of retriever? #The basic goal for you before launch is to optimize the recall and precision metrics. It‚Äôs basically the share of successful searches.\nRecall = Kfound / Krelevant\nPrecision = Kfound / Kretrieved\nTo calculate those metrics you could use simple dataset with 50 different questions and id‚Äôs of the most relevant chunks that should be found by the algorithm. Then you just run the search 50 times and compare chunks that were found and your relevant chunks.\nQuestion Relevant documents Find me a report about sales [1000, 1001, 1002] Paperclips profit [2004, 100] Then you could make this dataset bigger and more similar to your users questions.\nGeneration part #After that, I initialized LLM and pass those docs to the context of the LLM API call.\nAs for the prompt, I created this version (you can also expand it with additional context like a business overview or domain descriptions): Main instructions Answer template Business context You can use my code example and simply run it as CLI:\npython3 -m ask_me_anything -q \u0026#39;Find me a report about sales\u0026#39; ... üîé Relevant dashboards: - [Regional Dynamics](https://dl.ya.cloud/abc?tab=100) (278 views): Contains a table with sales data, including dimensions like region, city, and product, and measures such as GMV, sales, and average check. Confidence: 9/10 Some Tips # Pre-filter irrelevant metadata (outdated dashboards, somewhat from sandbox folder, etc.) to improve search efficiency. Sort your documents based on relevance by creating a custom retriever class. Consider your metadata and your user language when selecting the embedding model. To improve the quality of the retriever you could use re-phrases of the initial question and advanced algorithms (MMR and RRF) to make the recall better. Try to give your end users MVP on early stages: I guarantee they will surprise you with their questions :) Don‚Äôt hesitate to ask follow-up questions or reach out if you need advice.\nHere is a GitHub repository with all code and examples to start with.\nHappy coding! üöÄ\n","date":"12 June 2025","permalink":"https://vitaliykovalev.com/posts/how-to-apply-rag-and-llm-to-search-bi-reports-faster/","section":"Blog \u0026 Projects","summary":"A tool that takes report searching for business users to another level","title":"How to apply RAG and LLM to search BI reports faster"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/rag/","section":"Tags","summary":"","title":"RAG"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/project/","section":"Tags","summary":"","title":"Project"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/tableau/","section":"Tags","summary":"","title":"Tableau"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/tableau-makeover-monday-2025/","section":"Tags","summary":"","title":"Tableau Makeover Monday 2025"},{"content":"I‚Äôve been exploring the weekly energy mix in Great Britain‚Äôs grid from Jan 2024 to Jan 2025, focusing on how each source‚Äôs share shifts over time. You can see some notable fluctuations in renewables, fossil fuels, other sources, and transfers throughout the year ‚Äî possibly influenced by seasonal factors and changing demand.\nExplore more of my Tableau Projects here\n","date":"5 May 2025","permalink":"https://vitaliykovalev.com/posts/makeovermonday-2025-6/","section":"Blog \u0026 Projects","summary":"GB National Grid Energy Sources","title":"Tableau Makeover Monday 2025: Week 6"},{"content":"Anastasiya Kuznetsova mentioned my old TUG presentation (in Russian) where I shared a technique that provides a great way to multiply visuals without needing to recreate them.\nTL;DR: Embedding lets you reuse visuals across dashboards. It not only speeds up delivery but also improves load times, as embedded elements load at the same time.\nüîó Anastasiya‚Äôs example üîó My old example with comparison to default technique Each of those visuals are the same view but with different parameters How to do it? #There are basically two ways to embed something in Tableau:\nBasic JS API | Samples The JS API is for someone who wants to embed Tableau views and dashboards into another product, like a web page or application.\nHere, I will use the first method, assuming that we are able to embed one dashboard into another dashboard.\nFirst, let‚Äôs create a view or a dashboard that we plan to embed. It could be any type of visualisation. I created a KPI card with some additional context for my users and added a parameter to switch between regions:\nThere is 3 visualisations, but 1 worksheet Then I created a proper link and used it in the Web Page objects on the dashboard.\nThere are 5 Web Objects on the left, but only one visualisation is being reused Details #Let‚Äôs break it down using this example: https://your.tableau.server.com/#/views/ForEmbedding/Block?filter=some_value\u0026amp;multi_filter=value,value\u0026amp;all_values_in_filter=\u0026amp;:embed=yes\u0026amp;:toolbar=no\u0026amp;:showShareOptions=false\u0026amp;:showVizHome=no\nThere is some rules on how to create this link:\nMain link should lead to your worksheet on your Tableau Server / Online: https://your.tableau.server.com/#/views/\u0026lt;workbook\u0026gt;/\u0026lt;sheet\u0026gt; or https://your.tableau.server.com/#/site/\u0026lt;sitename\u0026gt;/views/\u0026lt;workbook\u0026gt;/\u0026lt;sheet\u0026gt; for multi-site environment. Tableau Public also compatible but link is slightly different, for example https://public.tableau.com/views/\u0026lt;workbook\u0026gt;/\u0026lt;sheet\u0026gt; Link should follow url-encode rules: query should begin with a question mark (?), separate the parameters using ampersands (\u0026amp;), separate multiple values per parameter using commas (,). Also, there is some reserved characters (,:/?#[]@!$\u0026amp;()*+,;='). You can pass parameters aliases using parameter=\u0026lt;parameter_name~na\u0026gt;. To link parameters with each other you can use (~s0), for example Region~s0=Central,West\u0026amp;Category~s0=Furniture,Technology will be recognised as \u0026quot;filter sales of Furniture category in Central region and Technology category in the West region\u0026quot;. Visual guide You can find the list of all available tech parameters here.\nThere is some disadvantages of this technique:\nIt does not work well with Tableau Public ‚Äî the toolbar stays visible and there‚Äôs no way to remove it Continuous filters are painful ‚Äî you can only pass values as a list, but you can create custom calculations to handle filtering Safari has a quirk ‚Äî adding 16px to the width fixes the scroll issue You need to encode spaces and other special characters in filter values (any online URL encoder can handle it) Filters aren‚Äôt supported inside Stories Overall, it‚Äôs a great feature, so feel free to use it if it fits your needs. Hope that helps you create better solutions for your customers.\nCheers!\n","date":"2 April 2025","permalink":"https://vitaliykovalev.com/posts/tableau-embedding/","section":"Blog \u0026 Projects","summary":"Great way to multiply visuals without needing to recreate them","title":"Embedding technique in Tableau"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/reporting/","section":"Tags","summary":"","title":"Reporting"},{"content":"There is a lot of data about Happiness. Here is my take on it:\nExplore more of my Tableau Projects here\n","date":"31 January 2025","permalink":"https://vitaliykovalev.com/posts/makeovermonday-2025-5/","section":"Blog \u0026 Projects","summary":"World Happiness Report 2024","title":"Tableau Makeover Monday 2025: Week 5"},{"content":"Finally jumping into #MakeoverMonday exploring current vs. preferred work arrangements:\nThe original visualization felt a bit too complex for me, so I simplified it with a slope chart + matrix approach.\nExplore more of my Tableau Projects here\n","date":"20 January 2025","permalink":"https://vitaliykovalev.com/posts/makeovermonday-2025-4/","section":"Blog \u0026 Projects","summary":"Visualisation exploring current vs. preferred work arrangements","title":"Tableau Makeover Monday 2025: Week 4"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/optimization/","section":"Tags","summary":"","title":"Optimization"},{"content":"Sometimes, we all get upset when our workbooks are slow. Additionally, it can be quite challenging to measure the exact loading time before and after some changes.\nThere is a hard way üò¨ #If you are using Tableau Server or Tableau Online you can use Admin Insights to monitor dashboard performance, but it is only available for administrators. Tableau Server admins could also use the internal Tableau database (workgroup) and Resource Management Tool to make report for all users, but it takes some time and skills. Also, it is only daily data refresh available for Tableau Online. If you want to measure the difference after workbook changes keep in mind that you would have this gap.\nAlternatively, you can use built-in Performance Recording to measure the exact loading time (check up my article on how to use it).\nBut there is a problem in all those ways. They are not really user-friendly and relatively slow or expensive to get some results right away.\nEasy way üòå #Many report creators I know use browser and stopwatch for those purposes. You simply open the report and measure time to it fully opens. The problem though is that you literally measure time with a naked eye.\nWell, not anymore! I am happy to introduce you Tableau Speedtest: a simple, easy-to-use tool that helps you measure your dashboard loading speed.\nüöÄ Check it out! Long story short: press Load Configs ‚Üí paste the link to your view in form ‚Üí press Run.\nMain features # Measure the exact time for every loading and the average of those test launches Get the list of views used on this dashboard There is no server-side: so you don‚Äôt have to worry about the security and NDA, the tool will not store any of your data, is handled by your browser (you can download the data in *.xls format though) Open-source (you can host it yourself) and free to use How to use it? #Step 1: Fill in the server config and press Load Configs You can pass this step and use the latest version of Tableau JS API available at Tableau Public site. Also, you can download JS API directly from your site (it could be useful if you don‚Äôt have Internet access from your Tableau Server or the network your company uses).\nStep 2: Enter the link to the report you want to test\nTo get a valid link on Tableau Public you can click on the share button and copy the link from the pop-up window. It should be a link to a particular dashboard, not a workbook link. Also, you can set the width and height of the iframe that will contain your report.\nStep 3: Click the Run button and wait ‚è≤ The tool will show how much time you spend waiting for the report loads and the report itself. Obliviously, you need to have access to the report and your Tableau Server / Online ;-) You can click the button Run Again to run a second test and so on. You will see all your test results in table and bar chart visualization. Also, the tool will calculate the average loading speed of your report. You can also download data right from the bar chart visualization (upper right corner menu ‚Üí Download).\nHow it works? #It is just simple javascript code that uses Tableau JS API to load the workbook and measure time. There is no backend service, all magic happens in your browser at your side.\nCan I contribute? #Of course! Any help will be appreciated! Contact me via email or simply create a new pull request here: https://github.com/vitaliy-kovalev/tableau-report-speedtest\nCheers!\n","date":"28 May 2024","permalink":"https://vitaliykovalev.com/posts/tableau-speedtest/","section":"Blog \u0026 Projects","summary":"I created a tool that could help you measure Tableau report speed","title":"Tableau Speedtest"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/tags/tools/","section":"Tags","summary":"","title":"Tools"},{"content":"Although Tableau is an easy-to-use drag-and-drop tool it can sometimes be challenging to understand why a report is so slow, particularly when dealing with a live database connection. The main goal of this article is to guide you through all kinds of optimisation tools and help you considerate which one to use.\nWhat tools do you have? #You can use Tableau Optimizer or Performance Recording tools to figure it out, but sometimes these may not be the most effective solutions. Additionally, there is Tableau Log Viewer, which I will introduce to you a bit later. The first tool (Server ‚Üí Run Optimizer) is great when you are trying to find some basic report design problems.\nOptimizer interface It covers 20 most common mistakes made by workbook creators: too many views on dashboard, unused fields, relative filters, etc. According to this tool one of the factors that making your workbooks slower is live connections:\nTableau extracts are designed to be as efficient as possible for use with analytics. Using extracts is one of the easiest ways to improve performance due to the data source. Extracts also have many native features for optimization.\nHowever, it always depends on the number of rows and columns you have: sometimes extracts is just not an option: tables with more than 1 billion rows and 200 columns are not so rare these days (hello, big data üòò).\nLet‚Äôs assume that you don‚Äôt have any other issues with the workbook except you couldn‚Äôt use extract for some reason.\nTo find out what‚Äôs wrong with our queries you can use another tool: Performance Recording\nIt demands some actions to not mess things up: you should press the button before you navigate to the dashboard. Otherwise, Tableau internal cache will affect the results. This is the main reason why you should not run it on Tableau Server (you can do it though) and use your local machine instead.\nYou should always keep in mind that internet connection speed and your machine CPU and RAM current load could affect the results. Always measure dashboard at least 3‚Äì4 times to make final decisions. How to run performance recording? # Close all unnecessary programs and clear database cache (if your database have one) Open Tableau Desktop Click Help ‚Üí Settings and Performance ‚Üí Run performance recording Open workbook you need to measure and navigate through all dashboards and views you want to measure Click Help ‚Üí Settings and Performance ‚Üí Stop performance recording If you have only one dataset and one dashboard you can run performance recording after opening your workbook (steps 3‚Äì5). To clear internal Tableau cache click Data ‚Üí {Your dataset name} ‚Üí Refresh\nWhen you close Tableau it will also clear all cache workbook has on Tableau side. You can take advantage of this feature as well.\nThe performance report will be displayed shortly after you stop the recording:\nPerformance recording report interface If you want to focus specifically on query optimisations you should pay attention only to the query execution events (green events). That‚Äôs where the queries can be found.\nThe downside of the performance recording method is that you have to re-run it after any changes are made on Tableau or the database side. This is when Tableau log viewer can be truly helpful.\nHow to install Tableau Log Viewer? #Go to GitHub and download the .dmg or .exe file from Releases page:\nWhere can you download TLV app You cannot simply run this tool on MacBooks with arm64 architecture (M1 / M2). To fix that open Terminal and install Rosetta 2:\nsoftwareupdate --install-rosetta In the process of the installation you should also type a and hit Enter.\nRosetta installation process Then you should drag TLV app to your Applications folder, right-click on that copy and click Get Info. Check the box Open using Rosetta and close info window. Also, you should make sure that app doesn‚Äôt have com.apple.quarantine attribute. To ensure that run this terminal command:\nxattr -dr com.apple.quarantine /Applications/tlv.app Finally, open the app itself. It should work just fine. You can keep this app in Dock and use it like a normal app.\nHere is the whole installation process:\nInstallation process and how to open log.txt How to use it? #Once you open the TLV app you should click the Open button and choose log.txt file from your ~/Documents/My Tableau Repository/Logs folder on macOS or C:\\Users\\\u0026lt;user\u0026gt;\\Documents\\My Tableau Repository\\Logs\\ on Windows.\nThen you will need to click Live mode button to continue to refresh info from Tableau Desktop.\nAnother option is to click Live capture ‚Üí Open log.txt\nAfter that you can make some actions in Tableau Desktop and you will see all actions that Tableau does in TLV app logs.\nI recommend you to use highlight mode available in the menu Highlight ‚Üí Highlight\nYou can add some filters in this window:\nEvent Name Description end-protocol.query SQL code and duration of single view queries qp-batch-summary Time needed to make a batch of requests begin-visual-controller.update-sheet View name end-visual-controller.update-visual-model Time needed to render views How to add new highlight You can save those filters Highlight ‚Üí Save and use them on the next use Highlight ‚Üí Load Filters\nHow to measure time properly? #You can use technique to eliminate Tableau internal cache similar to what you used when launching Performance Recording. Also, you can click Data ‚Üí {Datasource name} ‚Üí Refresh to clear the cache for a specific datasource. When you navigate to the dashboard or view TLV app will refresh the logs.\nI also recommend measuring views twice:\nClick on each view and then clear the cache Click on the dashboard with all views combined The behaviour of the views can vary depend on certain performance optimisation features of the Tableau, like query fusion (combining multiple individual queries into a single combined query) and join culling (simplifying queries by removing unnecessary joins).\nHow to read it? #Let‚Äôs assume you have this dashboard with 7 visualisations and 1 date filter:\nSimple dashboard That is what you get if you record how dashboard loads when you open it:\nThat‚Äôs how Tableau dashboard loads As you can see, there is quite a bit going on! Some queries will run in parallel, limited by your machine‚Äôs CPU usage and the number of connections the database can handle simultaneously. Keep in mind that the load time will be approximate.\nFurthermore, Tableau handles maps differently from other views: to load them, Tableau creates a temporary extract and stores it in memory to accelerate loading time.\nYou can also run all views one by one:\nNow all views have qp-batch-summary event with a relatively more accurate load time than end-prococol.query event provides. Additionally, it‚Äôs much easier to allocate query to the view. In first case you had to read the query itself to determine it.\nNow you can make some changes and view how it affects on performance. Also, you can measure events multiple times without any additional waiting time.\nOk, there is a bunch of queries but the code is not readable :( #Things like this is common for SQL-code generated by Tableau:\n(0_o) There‚Äôs another amazing tool Tableau Zen Parser made by Tony Liu that can parse all your workbook calculations and their ID‚Äôs based on your *.twb file.\nHow to use Tableau Zen Parser So if you have a lot of calculations and views based on them break your workspace to 3 pieces: Tableau, TLV and Zen Parser Results:\nWorkspace with Tableau, TLV and Zen Parser Now you can understand queries a lot easier using Calculated fields table as a dictionary.\nP.S. Tableau Log Viewer works also on Tableau Server! #You can log in to the machine running your Tableau Server and view server logs using TLV. These logs can help you debug issues with your integrations or Tableau components. You can find the actual locations of these server logs here.\nHope those tips and information were helpful. Please feel free to ask any additional questions in the comments section.\nHappy debugging!\n","date":"21 December 2023","permalink":"https://vitaliykovalev.com/posts/tableau-log-viewer/","section":"Blog \u0026 Projects","summary":"Complete guide of all kinds of optimisation tools in Tableau","title":"Tools overview to optimize Tableau reports"},{"content":"","date":null,"permalink":"https://vitaliykovalev.com/categories/","section":"Categories","summary":"","title":"Categories"}]